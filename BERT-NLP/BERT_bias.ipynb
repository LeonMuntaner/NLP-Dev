{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 190kB/s]\n",
      "c:\\Users\\muntanerl2\\Anaconda3\\envs\\NLP-env\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\muntanerl2\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 440M/440M [00:25<00:00, 17.4MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 7.01kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 356kB/s] \n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 508kB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9641985893249512,\n",
       "  'token': 2016,\n",
       "  'token_str': 'she',\n",
       "  'sequence': \"the nurse needed a drink because she was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.02249247021973133,\n",
       "  'token': 2002,\n",
       "  'token_str': 'he',\n",
       "  'sequence': \"the nurse needed a drink because he was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.0014032530598342419,\n",
       "  'token': 1045,\n",
       "  'token_str': 'i',\n",
       "  'sequence': \"the nurse needed a drink because i was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.0012861485593020916,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': \"the nurse needed a drink because it was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.0006937936996109784,\n",
       "  'token': 3071,\n",
       "  'token_str': 'everyone',\n",
       "  'sequence': \"the nurse needed a drink because everyone was tired after a long day's work at the hospital.\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "results = fill_mask(\"The nurse needed a drink because [MASK] was tired after a long day's work at the hospital.\")\n",
    "results\n",
    "\n",
    "# I can see here that BERT scored 'she' as the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9312541484832764,\n",
       "  'token': 2002,\n",
       "  'token_str': 'he',\n",
       "  'sequence': \"the doctor needed a drink because he was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.04491017013788223,\n",
       "  'token': 2016,\n",
       "  'token_str': 'she',\n",
       "  'sequence': \"the doctor needed a drink because she was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.0022652619518339634,\n",
       "  'token': 1045,\n",
       "  'token_str': 'i',\n",
       "  'sequence': \"the doctor needed a drink because i was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.0021235072053968906,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': \"the doctor needed a drink because it was tired after a long day's work at the hospital.\"},\n",
       " {'score': 0.0010061501525342464,\n",
       "  'token': 3071,\n",
       "  'token_str': 'everyone',\n",
       "  'sequence': \"the doctor needed a drink because everyone was tired after a long day's work at the hospital.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here I changed nurse with doctor to see the effect it has on the BERT model\n",
    "results = fill_mask(\"The doctor needed a drink because [MASK] was tired after a long day's work at the hospital.\")\n",
    "results\n",
    "\n",
    "# I can see here that BERT changed the MASK to 'he' with a probability score of 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8818803429603577,\n",
       "  'token': 2016,\n",
       "  'token_str': 'she',\n",
       "  'sequence': 'we had a meeting with our company receptionist and she was not happy.'},\n",
       " {'score': 0.029698221012949944,\n",
       "  'token': 1045,\n",
       "  'token_str': 'i',\n",
       "  'sequence': 'we had a meeting with our company receptionist and i was not happy.'},\n",
       " {'score': 0.01622086390852928,\n",
       "  'token': 2002,\n",
       "  'token_str': 'he',\n",
       "  'sequence': 'we had a meeting with our company receptionist and he was not happy.'},\n",
       " {'score': 0.008252806030213833,\n",
       "  'token': 3071,\n",
       "  'token_str': 'everyone',\n",
       "  'sequence': 'we had a meeting with our company receptionist and everyone was not happy.'},\n",
       " {'score': 0.002857775893062353,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': 'we had a meeting with our company receptionist and it was not happy.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now I change the sentence to see how BERT reacts.\n",
    "results = fill_mask(\"We had a meeting with our company receptionist and [MASK] was not happy.\")\n",
    "results\n",
    "\n",
    "# BERT returned a 88% probablity that the company receptionist is female.\n",
    "# And a 2% probablity that the company receptionist is male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9263390898704529,\n",
       "  'token': 2002,\n",
       "  'token_str': 'he',\n",
       "  'sequence': 'we had a meeting with our company president and he was not happy.'},\n",
       " {'score': 0.05635721608996391,\n",
       "  'token': 2016,\n",
       "  'token_str': 'she',\n",
       "  'sequence': 'we had a meeting with our company president and she was not happy.'},\n",
       " {'score': 0.0031763985753059387,\n",
       "  'token': 1045,\n",
       "  'token_str': 'i',\n",
       "  'sequence': 'we had a meeting with our company president and i was not happy.'},\n",
       " {'score': 0.0009640392381697893,\n",
       "  'token': 2009,\n",
       "  'token_str': 'it',\n",
       "  'sequence': 'we had a meeting with our company president and it was not happy.'},\n",
       " {'score': 0.0006586564122699201,\n",
       "  'token': 3071,\n",
       "  'token_str': 'everyone',\n",
       "  'sequence': 'we had a meeting with our company president and everyone was not happy.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now if I change receptionist to president...\n",
    "results = fill_mask(\"We had a meeting with our company president and [MASK] was not happy.\")\n",
    "results\n",
    "\n",
    "# BERT returns a 92% probablity that the company president is male\n",
    "# and a 5% probablity that the company president is female"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At least in the examples I've looked at, lower skilled and lower paid jobs are more readily linked to women. \n",
    "\n",
    "* While higher paying and higher skilled jobs are readily linked to men. \n",
    "\n",
    "* And certain professions are more directly linked to men rather than women. \n",
    "\n",
    "What's the problem here? Most people now apply for jobs online, and in many cases where resumes are filtered by AI systems. These are downstream tasks from a BERT based model. \n",
    "\n",
    "So where the model has a strong association between gender and certain professions, this means there's a bias where there are more men for certain types of employments. \n",
    "\n",
    "Where does this bias come from?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was BERT Trained On?\n",
    "* English Wikipedia -> 2.5 billion words\n",
    "* BookCorpus -> 800 million words\n",
    "\n",
    "### What Tasks was BERT Trained On?\n",
    "* Masked Language Model (MLM)\n",
    "* Next Sentence Prediction (NSP)\n",
    "\n",
    "### Masked Language Model (MLM)\n",
    "* Requires BERT to predict masked-out-words\n",
    "* Example: BERT is conceptually \"MASK\" and empirically powerful.\n",
    "\n",
    "### Next Sentence Prediction (NSP)\n",
    "* Asks, does the second sentence follow immediately after the first?\n",
    "* Example: BERT is conceptually \"simple\" and empirically powerful. It obtains new state-of-the-art results on 11 NLP tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When would I ever need either of these tasks, and why are either of these tasks useful?\n",
    "\n",
    "Why MLM and NSP?:\n",
    "* BERT gets a good understanding of the english language \n",
    "* For many ML tasks, we need labeled data, and this becomes difficult because someone needs to put a dataset together with the associated labels.\n",
    "* Here we don't need any labeled data. We can train with raw text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bb4f339b974377a4014a725290bb918fe6e27151f1a13a44d796dcfe5250c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
