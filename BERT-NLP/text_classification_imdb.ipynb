{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets is a library that allows easy access to different datasets.\n",
    "# These could be NLP tasks or computer vision or audio\n",
    "from datasets import list_datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "\n",
    "#list_datasets()\n",
    "pd.set_option('max_colwidth', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will be using the IMDB dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset('imdb')\n",
    "imdb\n",
    "\n",
    "# I can see that this is stored as a dataset dict, which is similar to a python dictionary.\n",
    "\n",
    "# Each key corresponds to a different split. These are train, test, and unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb['train'][0]\n",
    "# From the zero review I can tell that the rview was negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first three entries from the test split\n",
    "# I can also see that all of these are all negative \n",
    "imdb['test'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally I would like to work with the entire training dataset. But training this-\n",
    "# would take over two hours. \n",
    "\n",
    "# To help reduce this time I will reduce the dataset to only 2000 entries.\n",
    "imdb['train'] = imdb['train'].shuffle(seed=1).select(range(2000))\n",
    "imdb['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to take 1,600 of them for the training dataset and put 400 towards a validation-\n",
    "# dataset\n",
    "\n",
    "# The reason I want a validation dataset is that it will help me get an idea-\n",
    "# of how well the model is training. \n",
    "imdb_train_validation = imdb['train'].train_test_split(train_size=0.8)\n",
    "imdb_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_validation['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can now create my validation dataset\n",
    "imdb_train_validation['validation'] = imdb_train_validation.pop('test')\n",
    "imdb_train_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, because the dataset dict is like a python dictionary, I can use my newly formed-\n",
    "# IMDB train validation dataset dict and update the IMDB dataset dict with it. \n",
    "\n",
    "# This means that I will overwrite any current splits or keys with the same name.\n",
    "imdb.update(imdb_train_validation)\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now reduce the test set so that it has around 400 entries\n",
    "imdb['test'] = imdb['test'].shuffle(seed=1).select(range(400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the unsupervised values are not useful, I will delete them \n",
    "imdb.pop('unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the nice things about the huggingFace lib is that I can convert it to pandas-\n",
    "# so that I can visualize the dataset \n",
    "imdb.set_format('pandas')\n",
    "\n",
    "df = imdb['train'][:]\n",
    "df.sample(frac=1, random_state=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the 1st review\n",
    "df.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes the text has html tags and I want to remove this from the dataset\n",
    "df['text'] = df.text.str.replace('<br />', '')\n",
    "df.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to make sure that I have a balanced dataset. \n",
    "# This means that i want to have a similar ratio between positive and negative reviews \n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I will create a boxplot to see if I can see any patterns for whether reviews-\n",
    "# are labeled as a 0 or a 1.\n",
    "\n",
    "# I can see that I have a similar distribution for both\n",
    "\n",
    "from turtle import color\n",
    "\n",
    "\n",
    "df[\"Words per review\"] = df['text'].str.split().apply(len)\n",
    "df.boxplot(\"Words per review\", by=\"label\", grid=False, showfliers=False, color='black')\n",
    "\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at reviews that are less than 200 characters long\n",
    "# 0 = nagative | 1 = positive \n",
    "df[df.text.str.len() < 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the dataset back to the original form\n",
    "imdb.reset_format()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "* The next thing I want to do is tokenize the text so that I can convert the reviews from words to IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Creating a tokenize function and pass in an argument to the datasets map method\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
    "\n",
    "# The map method then applies the function to each element in the dataset.\n",
    "# imdb_encoded is now the dataset\n",
    "imdb_encoded = imdb.map(tokenize_function, batched=True, batch_size=None)\n",
    "imdb_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imdb_encoded['train'][0])\n",
    "\n",
    "# Now that I have the tokenized dataset, I can start to train my model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny IMDB\n",
    "\n",
    "* Now that I have a tokenized dataset, I will pass it through a BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show me the options I have for the AutoModel\n",
    "[x for x in dir(transformers) if re.search(r'^AutoModel', x)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a text classification problem, this falls under the 'AutoModelForSequenceClassification' model. What I'm doing here is adding a classification head on top of the pre-trained model with two classes. \n",
    "\n",
    "I will then be training this classification head as it will initially have random values. \n",
    "\n",
    "What's particularly helpful is that the AutoModel has a 'from_pretrain' method to load the weights of a pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# This line means that if I have a hardware accelerator, like a GPU, I want to-\n",
    "# use that rather than a CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# I will now specify that I have two labels in my dataset. \n",
    "num_labels = 2\n",
    "\n",
    "model = (AutoModelForSequenceClassification\n",
    "        .from_pretrained(checkpoint, num_labels=num_labels)\n",
    "        .to(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I'm training a model, I like to take a really small sample of the data, train with that, and see if I'm getting the output that I expect.\n",
    "\n",
    "If I'm happy with that then I go ahead and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the IMDB dataset that I've been using and create what I will call -\n",
    "# the tiny_imdb\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "tiny_imdb = DatasetDict()\n",
    "\n",
    "# This will only have 50 examples to train from and 10 each for the validation and test split.\n",
    "tiny_imdb['train'] = imdb['train'].shuffle(seed=1).select(range(50))\n",
    "tiny_imdb['validation'] = imdb['validation'].shuffle(seed=1).select(range(10))\n",
    "tiny_imdb['test'] = imdb['test'].shuffle(seed=1).select(range(10))\n",
    "\n",
    "# I'll then go ahead and encode the dataset \n",
    "tiny_imdb_encoded = tiny_imdb.map(tokenize_function, batched=True, batch_size=None)\n",
    "tiny_imdb_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hugging face library makes it easy to train a model using the Trainer, and TrainingArguments class. \n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "logging_steps = len(tiny_imdb_encoded['train']) // batch_size\n",
    "model_name = f\"{checkpoint}-finetuned-tiny-imdb\"\n",
    "\n",
    "# I can specify the training parameters \n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  optim=\"adamw_torch\",)\n",
    "\n",
    "training_args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tiny_imdb_encoded['train'],\n",
    "                  eval_dataset=tiny_imdb_encoded['validation'],\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train();\n",
    "# I want to see a column showing the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(tiny_imdb_encoded['test'])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will now extract only the predictions component from the preds\n",
    "# The output of this is a tensor. \n",
    "\n",
    "preds.predictions.argmax(axis=-1)\n",
    "# These are all of the predictions from my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also have the actual labels, I can also grab those here. \n",
    "preds.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I have both the predictions and the actual labels, I can use-\n",
    "# the accuracy_score function from scikit-learn \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function named get_accuracy\n",
    "# This will return a dictionary which includes the accuracy.\n",
    "def get_accuracy(preds):\n",
    "    predictions = preds.predictions.argmax(axis=-1)\n",
    "    labels = preds.label_ids\n",
    "    accuracy = accuracy_score(preds.label_ids, preds.predictions.argmax(axis=-1))\n",
    "    return {'accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  compute_metrics=get_accuracy,\n",
    "                  args=training_args,\n",
    "                  train_dataset=tiny_imdb_encoded['train'],\n",
    "                  eval_dataset=tiny_imdb_encoded['validation'],\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "logging_steps = len(imdb_encoded['train']) // batch_size\n",
    "model_name = f\"{checkpoint}-finetuned-imdb\"\n",
    "\n",
    "# I can specify the training parameters \n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  optim=\"adamw_torch\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take 2 hours on this machine\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=get_accuracy,\n",
    "                  train_dataset=imdb_encoded['train'],\n",
    "                  eval_dataset=imdb_encoded['validation'],\n",
    "                  tokenizer=tokenizer)\n",
    "\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out the model\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model=model_name)\n",
    "classifier('This is not my idea of fun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('This was an amazing experience')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bb4f339b974377a4014a725290bb918fe6e27151f1a13a44d796dcfe5250c51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
